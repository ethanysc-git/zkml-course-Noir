{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset from Keras\n",
    "mnist = keras.datasets.mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data() # load dataset\n",
    "\n",
    "X_train_full.astype(np.float32)\n",
    "y_train_full.astype(np.float32)\n",
    "X_test.astype(np.float32)\n",
    "y_test.astype(np.float32)\n",
    "\n",
    "# Scale pixel intensities between 0-1 and create a validation set\n",
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0 \n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 30)                3030      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                310       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 268940 (1.03 MB)\n",
      "Trainable params: 268940 (1.03 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexjaniak/Programs/miniforge3/envs/zkMNIST/lib/python3.11/site-packages/keras/src/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Create & compile simple sequential model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\") # output\n",
    "])\n",
    "\n",
    "# Compile Model\n",
    "model.compile(\n",
    "    loss=keras.losses.sparse_categorical_crossentropy, # sparse labels\n",
    "    optimizer=keras.optimizers.legacy.SGD(lr=0.01, clipvalue = 0.5),\n",
    "    metrics=[keras.metrics.sparse_categorical_accuracy]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.6974 - sparse_categorical_accuracy: 0.8070 - val_loss: 0.2951 - val_sparse_categorical_accuracy: 0.9154\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2690 - sparse_categorical_accuracy: 0.9226 - val_loss: 0.2187 - val_sparse_categorical_accuracy: 0.9374\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2095 - sparse_categorical_accuracy: 0.9397 - val_loss: 0.1809 - val_sparse_categorical_accuracy: 0.9480\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1719 - sparse_categorical_accuracy: 0.9510 - val_loss: 0.1585 - val_sparse_categorical_accuracy: 0.9550\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1464 - sparse_categorical_accuracy: 0.9577 - val_loss: 0.1334 - val_sparse_categorical_accuracy: 0.9616\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.1266 - sparse_categorical_accuracy: 0.9641 - val_loss: 0.1252 - val_sparse_categorical_accuracy: 0.9636\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.1111 - sparse_categorical_accuracy: 0.9679 - val_loss: 0.1125 - val_sparse_categorical_accuracy: 0.9672\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.0980 - sparse_categorical_accuracy: 0.9718 - val_loss: 0.1037 - val_sparse_categorical_accuracy: 0.9718\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0878 - sparse_categorical_accuracy: 0.9746 - val_loss: 0.0961 - val_sparse_categorical_accuracy: 0.9738\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0780 - sparse_categorical_accuracy: 0.9770 - val_loss: 0.0951 - val_sparse_categorical_accuracy: 0.9724\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0703 - sparse_categorical_accuracy: 0.9799 - val_loss: 0.0864 - val_sparse_categorical_accuracy: 0.9752\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0637 - sparse_categorical_accuracy: 0.9817 - val_loss: 0.0911 - val_sparse_categorical_accuracy: 0.9752\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0573 - sparse_categorical_accuracy: 0.9838 - val_loss: 0.0830 - val_sparse_categorical_accuracy: 0.9760\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0516 - sparse_categorical_accuracy: 0.9851 - val_loss: 0.0793 - val_sparse_categorical_accuracy: 0.9758\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0467 - sparse_categorical_accuracy: 0.9867 - val_loss: 0.0838 - val_sparse_categorical_accuracy: 0.9750\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0423 - sparse_categorical_accuracy: 0.9881 - val_loss: 0.0771 - val_sparse_categorical_accuracy: 0.9782\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0378 - sparse_categorical_accuracy: 0.9894 - val_loss: 0.0747 - val_sparse_categorical_accuracy: 0.9790\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0343 - sparse_categorical_accuracy: 0.9911 - val_loss: 0.0775 - val_sparse_categorical_accuracy: 0.9788\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0305 - sparse_categorical_accuracy: 0.9923 - val_loss: 0.0783 - val_sparse_categorical_accuracy: 0.9770\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0283 - sparse_categorical_accuracy: 0.9926 - val_loss: 0.0728 - val_sparse_categorical_accuracy: 0.9792\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0248 - sparse_categorical_accuracy: 0.9938 - val_loss: 0.0783 - val_sparse_categorical_accuracy: 0.9776\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0227 - sparse_categorical_accuracy: 0.9946 - val_loss: 0.0754 - val_sparse_categorical_accuracy: 0.9792\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0203 - sparse_categorical_accuracy: 0.9957 - val_loss: 0.0751 - val_sparse_categorical_accuracy: 0.9802\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0183 - sparse_categorical_accuracy: 0.9959 - val_loss: 0.0735 - val_sparse_categorical_accuracy: 0.9800\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.0165 - sparse_categorical_accuracy: 0.9971 - val_loss: 0.0764 - val_sparse_categorical_accuracy: 0.9790\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0146 - sparse_categorical_accuracy: 0.9975 - val_loss: 0.0751 - val_sparse_categorical_accuracy: 0.9790\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0133 - sparse_categorical_accuracy: 0.9979 - val_loss: 0.0758 - val_sparse_categorical_accuracy: 0.9796\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0122 - sparse_categorical_accuracy: 0.9982 - val_loss: 0.0741 - val_sparse_categorical_accuracy: 0.9810\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0106 - sparse_categorical_accuracy: 0.9989 - val_loss: 0.0748 - val_sparse_categorical_accuracy: 0.9816\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0099 - sparse_categorical_accuracy: 0.9988 - val_loss: 0.0765 - val_sparse_categorical_accuracy: 0.9798\n"
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 15.4702 - sparse_categorical_accuracy: 0.9752\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[15.470162391662598, 0.9751999974250793]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 30)                3030      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 268630 (1.02 MB)\n",
      "Trainable params: 268630 (1.02 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create new model without the output layer\n",
    "frontend = keras.models.Sequential(model.layers[:-1])\n",
    "\n",
    "# Copy the weights and biases from the original model to the new model\n",
    "for layer, new_layer in zip(model.layers[:-1], frontend.layers):\n",
    "    new_layer.set_weights(layer.get_weights())\n",
    "\n",
    "frontend.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "-------------------------------\n",
      "Original Model Output Shape:  (2, 10)\n",
      "Original Model Output Values: [[8.7689613e-12 3.6768448e-08 1.7953114e-07 1.6368752e-02 3.4092315e-17\n",
      "  9.8363101e-01 1.7702296e-12 2.6962912e-10 2.0212696e-11 1.0865001e-08]\n",
      " [9.9999619e-01 1.4972997e-12 3.5517605e-06 5.1023018e-12 5.6060295e-11\n",
      "  2.4501759e-12 4.1206292e-08 1.9631443e-07 8.7891777e-11 1.6871030e-10]]\n",
      "Original Model Prediction:    [5 0]\n",
      "                              \n",
      "New Model Output Shape:       (2, 10)\n",
      "New Model Output Values:      [[8.7689613e-12 3.6768448e-08 1.7953114e-07 1.6368752e-02 3.4092315e-17\n",
      "  9.8363101e-01 1.7702296e-12 2.6962912e-10 2.0212696e-11 1.0865001e-08]\n",
      " [9.9999619e-01 1.4972997e-12 3.5517605e-06 5.1023018e-12 5.6060295e-11\n",
      "  2.4501759e-12 4.1206292e-08 1.9631443e-07 8.7891777e-11 1.6871030e-10]]\n",
      "New Model Prediction:         [5 0]\n",
      "                              \n",
      "Expected Ouput:               [5 0]\n",
      "-------------------------------\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Manually compute the output of the new model using the last layer's weights and biases from the original model\n",
    "last_layer_weights, last_layer_biases = model.layers[-1].get_weights()\n",
    "frontend_output = tf.matmul(frontend.predict(X_valid[:2]), last_layer_weights) + last_layer_biases\n",
    "frontend_output = tf.nn.softmax(frontend_output)\n",
    "\n",
    "model_output = model.predict(X_valid[:2])\n",
    "\n",
    "# Prepare the data for the table\n",
    "table_data = [\n",
    "    [\"Original Model Output Shape:\", model_output.shape],\n",
    "    [\"Original Model Output Values:\", model_output],\n",
    "    [\"Original Model Prediction:\", np.argmax(model_output, 1)],\n",
    "    [\"\", \"\"],  # Empty row for spacing\n",
    "    [\"New Model Output Shape:\", frontend_output.shape],\n",
    "    [\"New Model Output Values:\", frontend_output.numpy()],\n",
    "    [\"New Model Prediction:\", np.argmax(frontend_output, 1)],\n",
    "    [\"\", \"\"],\n",
    "    [\"Expected Ouput:\", y_valid[:2]]\n",
    "]\n",
    "\n",
    "# Determine the maximum length of the descriptions for alignment\n",
    "max_desc_length = max(len(row[0]) for row in table_data)\n",
    "\n",
    "# Print Test Results\n",
    "print(\"-\" * (max_desc_length + 2))\n",
    "for row in table_data:\n",
    "    description = row[0].ljust(max_desc_length)\n",
    "    value = row[1]\n",
    "    print(f\"{description} {value}\")\n",
    "print(\"-\" * (max_desc_length + 2))\n",
    "\n",
    "\n",
    "print(frontend_output.numpy().all() == model_output.all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.4746044 , -0.17401358,  0.04045997,  0.49065602,  0.34011176,\n",
       "          0.37476963, -0.5948454 , -0.21871452, -0.4372033 ,  0.57128954],\n",
       "        [ 0.32773218, -0.69739807,  0.47574416,  0.37404647, -0.90683013,\n",
       "          0.13982633, -0.530216  ,  0.4522029 , -0.64315176, -0.04918684],\n",
       "        [-0.02300171, -0.5227803 ,  0.46335906, -0.22812063, -0.11517239,\n",
       "          0.03219993,  0.6284023 , -0.751931  ,  0.57648057,  0.03546061],\n",
       "        [ 0.02755888,  0.7532413 ,  0.67983705, -0.02399273,  0.15439917,\n",
       "         -0.3311136 ,  0.41011298, -0.00824951, -0.42705557, -0.86243606],\n",
       "        [-0.32337558, -0.00815772,  0.49542132, -0.74162865,  0.6949287 ,\n",
       "         -0.01529182,  0.2705112 ,  0.87966275, -0.251072  , -0.5111604 ],\n",
       "        [ 0.853414  , -0.08771293, -0.3074586 , -0.58547544, -0.54565567,\n",
       "         -0.7266197 ,  0.62607574,  0.4645215 , -0.56800693, -0.21421501],\n",
       "        [-0.40069675, -0.45823002,  0.3586777 ,  0.7629971 ,  0.03995378,\n",
       "          0.43127215,  0.37921515, -0.34431472, -0.40455553, -0.63211304],\n",
       "        [-0.38321564, -0.19170015, -0.11195551,  0.25323224,  0.4815436 ,\n",
       "         -0.08236158,  0.38564238,  0.00203707, -0.22248682, -0.6541043 ],\n",
       "        [-0.32544816,  0.3175052 , -0.44453993, -0.10423727, -0.1824006 ,\n",
       "         -0.4834933 , -0.0234205 ,  0.59703356, -0.6855766 ,  0.5846635 ],\n",
       "        [-0.07227375,  0.6433285 , -0.32258046,  0.7160897 , -0.77292496,\n",
       "          0.30147135, -0.24581935,  0.2035053 ,  0.4366716 , -0.6155226 ],\n",
       "        [ 0.6293815 , -0.75508296, -0.02200869, -0.16038382,  0.15566888,\n",
       "         -0.4175784 , -0.6457767 ,  0.4390426 ,  0.8103376 , -0.24673894],\n",
       "        [-0.2897336 ,  0.35131302, -0.07959925, -0.17117202,  0.05226851,\n",
       "         -0.1823485 , -0.40750042,  0.41125426, -0.00294888, -0.42608827],\n",
       "        [-0.5742528 ,  0.51040703,  0.27096426,  0.45163634, -0.13102621,\n",
       "         -0.49473014, -0.33640137, -0.2945637 ,  0.4074374 , -0.12160324],\n",
       "        [-0.22924948, -0.13028078,  0.09420767, -0.38986224, -0.53752786,\n",
       "         -0.1734073 ,  0.65717465,  0.36076263,  0.48537293, -0.39594394],\n",
       "        [-0.6163116 , -0.17726596, -0.554899  ,  0.7715047 ,  0.48170662,\n",
       "         -0.5660075 , -0.6015993 ,  0.11565432,  0.66770035,  0.43062904],\n",
       "        [ 0.09331936,  0.33426857, -0.886796  , -0.9079123 ,  0.5191836 ,\n",
       "          0.73994213,  0.7740517 , -0.38685906, -0.2108202 ,  0.02201604],\n",
       "        [ 0.14977986,  0.09152729,  0.33620715, -0.3208609 , -0.05076704,\n",
       "          0.18279283, -0.28870958, -0.37245092, -0.02779945, -0.02836199],\n",
       "        [-0.63634706,  0.18117402,  0.22358967, -0.38619512, -0.6335041 ,\n",
       "          0.25296223,  0.13225515,  0.73609614, -0.42577952,  0.61368716],\n",
       "        [-0.1559635 , -0.2989252 ,  0.23981169,  0.00158379, -0.4884523 ,\n",
       "         -0.4729277 ,  0.1431926 , -0.20058027, -0.47311985,  0.3201213 ],\n",
       "        [ 0.36654383, -0.30937806, -0.14053065,  0.35718885,  0.36995623,\n",
       "          0.76756364, -0.23988573, -0.11013841, -0.49547258,  0.1955742 ],\n",
       "        [-0.06096966, -0.03339257,  0.12150431, -0.31063506,  0.6142998 ,\n",
       "          0.06032321, -0.53975224,  0.3044194 ,  0.11496086,  0.22422236],\n",
       "        [ 0.1137341 ,  0.32829162,  0.19285561, -0.16468392,  0.36589718,\n",
       "          0.03115144,  0.368622  , -0.37386405, -0.36061284, -0.35883278],\n",
       "        [ 0.5215923 ,  0.36204678,  0.26258123,  0.43821537,  0.04806282,\n",
       "          0.0529296 , -0.5169565 , -0.6328358 , -0.0070883 , -0.3801109 ],\n",
       "        [ 0.1493574 , -0.44925976,  0.23723127, -0.22238566,  0.5192028 ,\n",
       "         -0.96088   , -0.42432284, -0.2626076 ,  0.10169058,  0.6731891 ],\n",
       "        [ 0.02479562,  0.7650432 ,  0.88824046, -0.43553227,  0.05029394,\n",
       "          0.23454207, -0.29846686,  0.02605952, -0.565162  ,  0.16649622],\n",
       "        [-0.0545386 ,  0.27752233, -0.4826338 ,  0.5259284 , -0.32834432,\n",
       "         -0.39737168,  0.17209716, -0.44809306, -0.4456307 ,  0.44698387],\n",
       "        [-0.49160653,  0.01913467, -0.49483082, -0.52494955, -0.7952242 ,\n",
       "          0.9540691 , -0.09467641, -0.9188647 ,  0.7422298 ,  0.7522864 ],\n",
       "        [ 0.22810215,  0.24713208,  0.1950128 ,  0.28087136,  0.07199332,\n",
       "         -0.06520052,  0.23606803,  0.08589591,  0.07048725,  0.10496897],\n",
       "        [ 0.34987983,  0.67466307, -0.4051191 , -0.03577373,  0.30373815,\n",
       "         -0.12740748,  0.15378474, -0.11064661, -0.2700081 ,  0.07451992],\n",
       "        [-0.09466448, -0.1604384 ,  0.0487087 , -0.01335839, -0.30559614,\n",
       "         -0.10202537,  0.34311226, -0.11963018, -0.0941559 ,  0.36179507]],\n",
       "       dtype=float32),\n",
       " array([-0.00846264,  0.04778683, -0.04457129, -0.07094951, -0.05385553,\n",
       "         0.21029502, -0.09301671,  0.02082468, -0.01764383,  0.00959148],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[-1].get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step\n",
      "scaled weights: [99999526, 100000327, 99999977, 100000027, 99999677, 100000853, 99999600, 99999617, 99999675, 99999928, 100000629, 99999711, 99999426, 99999771, 99999384, 100000093, 100000149, 99999364, 99999845, 100000366, 99999940, 100000113, 100000521, 100000149, 100000024, 99999946, 99999509, 100000228, 100000349, 99999906, 99999826, 99999303, 99999478, 100000753, 99999992, 99999913, 99999542, 99999809, 100000317, 100000643, 99999245, 100000351, 100000510, 99999870, 99999823, 100000334, 100000091, 100000181, 99999702, 99999691, 99999967, 100000328, 100000362, 99999551, 100000765, 100000277, 100000019, 100000247, 100000674, 99999840, 100000040, 100000475, 100000463, 100000679, 100000495, 99999693, 100000358, 99999889, 99999556, 99999678, 99999978, 99999921, 100000270, 100000094, 99999446, 99999114, 100000336, 100000223, 100000239, 99999860, 100000121, 100000192, 100000262, 100000237, 100000888, 99999518, 99999506, 100000195, 99999595, 100000048, 100000490, 100000374, 99999772, 99999977, 99999259, 99999415, 100000762, 100000253, 99999896, 100000716, 99999840, 99999829, 100000451, 99999611, 100000771, 99999093, 99999680, 99999614, 100000001, 100000357, 99999690, 99999836, 100000438, 99999778, 99999565, 100000525, 99999476, 100000280, 99999965, 99999987, 100000340, 99999094, 99999885, 100000154, 100000694, 99999455, 100000039, 100000481, 99999818, 99999228, 100000155, 100000052, 99999869, 99999463, 100000481, 100000519, 99999950, 99999367, 99999512, 100000369, 100000614, 100000365, 100000048, 100000519, 100000050, 99999672, 99999205, 100000071, 100000303, 99999695, 100000374, 100000139, 100000032, 99999669, 99999985, 99999274, 100000431, 99999918, 99999517, 100000301, 99999583, 99999818, 99999506, 99999827, 99999434, 100000739, 100000182, 100000252, 99999528, 100000767, 100000060, 100000031, 100000052, 99999040, 100000234, 99999603, 100000954, 99999935, 99999873, 99999898, 99999406, 99999470, 100000628, 100000410, 100000270, 100000626, 100000379, 100000385, 99999977, 99999755, 99999355, 99999593, 99999664, 100000657, 99999399, 100000774, 99999712, 100000132, 100000143, 99999761, 99999461, 100000368, 99999484, 99999576, 99999702, 100000172, 99999906, 100000236, 100000153, 100000343, 99999782, 100000452, 99999249, 99999992, 100000879, 100000464, 99999656, 100000002, 100000597, 100000203, 100000439, 100000411, 99999706, 100000360, 100000115, 99999614, 99999628, 100000736, 99999800, 99999890, 100000304, 99999627, 99999368, 99999738, 100000026, 99999552, 99999082, 100000085, 99999890, 99999881, 99999563, 99999357, 100000576, 99999573, 99999749, 99999432, 99999596, 99999778, 99999315, 100000436, 100000810, 99999998, 100000407, 100000485, 100000667, 99999790, 99999973, 99999575, 99999527, 99999505, 100000114, 99999640, 99999993, 100000101, 99999435, 99999555, 100000742, 100000070, 99999730, 99999906, 100000571, 99999951, 100000035, 99999138, 99999489, 99999786, 99999368, 99999346, 100000584, 99999385, 99999754, 99999574, 99999879, 99999605, 100000430, 100000022, 99999972, 100000613, 100000320, 100000195, 100000224, 99999642, 99999620, 100000673, 100000166, 100000446, 100000752, 100000104, 100000074, 100000361]\n",
      "scaled input: [0, 77298, 141847, 1096636, 592410, 284597, 121221, 181959, 458342, 1466792, 534306, 0, 837217, 675388, 679225, 862028, 0, 343306, 0, 0, 318670, 0, 672436, 404488, 909243, 0, 305420, 0, 483334, 0]\n",
      "scaled bias: [99991538, 100047786, 99955429, 99929051, 99946145, 100210295, 99906984, 100020824, 99982357, 100009591]\n",
      "matmul output: [1144615606594375, 1144619210558846, 1144616685939125, 1144615978984581, 1144616069251694, 1144615688650303, 1144616088084889, 1144616888090542, 1144616671664825, 1144614688644202]\n",
      "output: [1144615706585913, 1144619310606632, 1144616785894554, 1144616078913632, 1144616169197839, 1144615788860598, 1144616187991873, 1144616988111366, 1144616771647182, 1144614788653793]\n",
      "argmax: 1\n"
     ]
    }
   ],
   "source": [
    "# Manually Compute \n",
    "\n",
    "BIG_INT = 100000000 # future use: 2**32\n",
    "\n",
    "# scale the input, weights, & baises to remove floats\n",
    "scaled_input = frontend.predict(X_test[2:3])*1000 # input is positive due to ReLu\n",
    "scaled_input = scaled_input.astype(int) # floor\n",
    "#scaled_input += BIG_INT \n",
    "\n",
    "scaled_weights = last_layer_weights*1000\n",
    "scaled_weights = scaled_weights.astype(int) # floor \n",
    "scaled_weights += BIG_INT\n",
    "\n",
    "scaled_bias = last_layer_biases*1000*1000\n",
    "scaled_bias = scaled_bias.astype(int)\n",
    "scaled_bias += BIG_INT\n",
    "\n",
    "output1 = tf.matmul(scaled_input, scaled_weights)\n",
    "output2 = np.add(output1, scaled_bias)\n",
    "output3 = np.argmax(output2)\n",
    "\n",
    "print(\"scaled weights:\", scaled_weights.transpose().flatten().tolist())\n",
    "print(\"scaled input:\", scaled_input.flatten().tolist())\n",
    "print(\"scaled bias:\", scaled_bias.tolist())\n",
    "print(\"matmul output:\", output1.numpy().astype(int).flatten().tolist())\n",
    "print(\"output:\", output2.astype(int).flatten().tolist())\n",
    "print(\"argmax:\", output3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexjaniak/Programs/miniforge3/envs/zkMNIST/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import tensorflowjs as tfjs\n",
    "\n",
    "# Save Keras model for tfjs\n",
    "tfjs.converters.save_keras_model(model, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexjaniak/Programs/miniforge3/envs/zkMNIST/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save Keras model for model_test.ipynb\n",
    "model.save(\"model/model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
